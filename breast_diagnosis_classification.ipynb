{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>0</td><td>application_1543193125837_0001</td><td>pyspark3</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-56-150.ec2.internal:20888/proxy/application_1543193125837_0001/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-57-203.ec2.internal:8042/node/containerlogs/container_1543193125837_0001_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import StringIndexer,IndexToString\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "# from pyspark.ml.feature import IDF\n",
    "# from pyspark.ml.feature import DCT\n",
    "# from pyspark.ml.feature import PolynomialExpansion\n",
    "# from pyspark.ml.feature import ChiSqSelector\n",
    "from pyspark.ml import Pipeline\n",
    "import itertools as it\n",
    "import pyspark.sql.functions as f\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 1. rename column names containing '.' (GS1-279B7.1, GS1-600G8.3 CAND1.11, HY.1)\n",
    "#      2. read csv sep = ',' for cpv final matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"PrimaryBreastApp\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'driverMemory': '1000M', 'executorCores': 2, 'proxyUser': 'jovyan', 'kind': 'pyspark3'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>0</td><td>application_1543193125837_0001</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-56-150.ec2.internal:20888/proxy/application_1543193125837_0001/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-57-203.ec2.internal:8042/node/containerlogs/container_1543193125837_0001_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SparkContext.setSystemProperty('spark.driver.memory', '15g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SparkContext.setSystemProperty('spark.executor.memory', '15g')\n",
    "#sc._conf.getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to get all stored variables\n",
    "def list_dataframes():\n",
    "    from pyspark.sql import DataFrame\n",
    "    return [k for (k, v) in globals().items() if isinstance(v, DataFrame)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from botocore.exceptions import ClientError\n",
    "\n",
    "def check(s3, bucket, key):\n",
    "    try:\n",
    "        s3.head_object(Bucket=bucket, Key=key)\n",
    "    except ClientError as e:\n",
    "        return int(e.response['Error']['Code']) != 404\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTrainingMetrics(trainingSummary,printout=True):\n",
    "    # for multiclass, we can inspect metrics on a per-label basis\n",
    "#     print(\"False positive rate by label:\")\n",
    "#     for i, rate in enumerate(trainingSummary.falsePositiveRateByLabel):\n",
    "#         print(\"label %d: %s\" % (i, rate))\n",
    "\n",
    "#     print(\"True positive rate by label:\")\n",
    "#     for i, rate in enumerate(trainingSummary.truePositiveRateByLabel):\n",
    "#         print(\"label %d: %s\" % (i, rate))\n",
    "\n",
    "#     print(\"Precision by label:\")\n",
    "#     for i, prec in enumerate(trainingSummary.precisionByLabel):\n",
    "#         print(\"label %d: %s\" % (i, prec))\n",
    "\n",
    "#     print(\"Recall by label:\")\n",
    "#     for i, rec in enumerate(trainingSummary.recallByLabel):\n",
    "#         print(\"label %d: %s\" % (i, rec))\n",
    "\n",
    "    print(\"F-measure by label:\")\n",
    "    for i, f in enumerate(trainingSummary.fMeasureByLabel()):\n",
    "        print(\"label %d: %s\" % (i, f))\n",
    "\n",
    "    accuracy = trainingSummary.accuracy\n",
    "    falsePositiveRate = trainingSummary.weightedFalsePositiveRate\n",
    "    truePositiveRate = trainingSummary.weightedTruePositiveRate\n",
    "    fMeasure = trainingSummary.weightedFMeasure()\n",
    "    precision = trainingSummary.weightedPrecision\n",
    "    recall = trainingSummary.weightedRecall\n",
    "    if printout is True:\n",
    "        print(\"Accuracy: %s\\nFPR: %s\\nTPR: %s\\nF-measure: %s\\nPrecision: %s\\nRecall: %s\"\n",
    "          % (accuracy, falsePositiveRate, truePositiveRate, fMeasure, precision, recall))\n",
    "    return {\"accuracy\": accuracy, \"fpr\": falsePositiveRate, \"tpr\": truePositiveRate, \"fmeasure\": fMeasure, \\\n",
    "            \"precision\": precision, \"recall\": recall}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data file\n",
    "mirna_path = 's3://gdc-emr0/mirna_filtered_matrix.csv'\n",
    "cpv_path = 's3://gdc-emr0/cpv_filtered_matrix.csv'\n",
    "# mrna_path = 's3://gdc-emr0/mrna_filtered_matrix.csv'\n",
    "# read mirna\n",
    "df_mirna = spark.read.option(\"maxColumns\", 22400).csv(\n",
    "    mirna_path, header=True, sep = ',',mode=\"DROPMALFORMED\",inferSchema=True)\n",
    "# read cpv\n",
    "df_cpv = spark.read.option(\"maxColumns\", 22400).csv(\n",
    "    cpv_path, header=True, sep = ',',mode=\"DROPMALFORMED\",inferSchema=True)\n",
    "# # read mrna\n",
    "# df_mrna = spark.read.option(\"maxColumns\", 22400).csv(\n",
    "#     mrna_path, header=True, sep = ',',mode=\"DROPMALFORMED\",inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cpv = df_cpv.toDF(*(c.replace('.', '_') for c in df_cpv.columns))\n",
    "# df_mrna = df_mrna.toDF(*(c.replace('.', '_') for c in df_mrna.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group columns by label, identifier and feature\n",
    "label_columns = ['sample_type', 'disease_type', 'primary_diagnosis']\n",
    "mirna_identifier_columns = ['sample_id','case_id']\n",
    "mirna_feature_columns = [x for x in df_mirna.columns if x not in (label_columns+mirna_identifier_columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group columns by label, identifier and feature\n",
    "cpv_identifier_columns= ['_c0','sample_id','case_id']\n",
    "cpv_feature_columns = [x for x in df_cpv.columns if x not in (label_columns+cpv_identifier_columns)]\n",
    "df_cpv = df_cpv.withColumnRenamed(\"sample_type\", \"sample_type_cpv\").withColumnRenamed(\"disease_type\", \"disease_type_cpv\").withColumnRenamed(\"primary_diagnosis\",\"primary_diagnosis_cpv\").withColumnRenamed(\"case_id\", \"case_id_cpv\")\n",
    "cpv_label_columns = ['sample_type_cpv', 'disease_type_cpv', 'primary_diagnosis_cpv']\n",
    "cpv_identifier_columns=['_c0','sample_id','case_id_cpv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # group columns by label, identifier and feature\n",
    "# # cpv_label_columns = ['sample_type', 'disease_type', 'primary_diagnosis']\n",
    "# mrna_identifier_columns= ['_c0','sample_id','case_id']\n",
    "# mrna_feature_columns = [x for x in df_mrna.columns if x not in (label_columns+mrna_identifier_columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert features into (sparse) vectors\n",
    "# mirna\n",
    "assembler = VectorAssembler(inputCols=mirna_feature_columns, outputCol='features_mirna')\n",
    "df_mirna = assembler.transform(df_mirna)\n",
    "df_mirna=df_mirna.drop(*mirna_feature_columns)\n",
    "# eventually we should store/load from HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cpv\n",
    "assembler = VectorAssembler(inputCols=cpv_feature_columns, outputCol='features_cpv')\n",
    "df_cpv = assembler.transform(df_cpv)\n",
    "df_cpv = df_cpv.drop(*cpv_feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_cpv.join(df_mirna, on=['sample_id'], how='left_outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_breast = df.where(df.disease_type_cpv == 'Breast Invasive Carcinoma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1075"
     ]
    }
   ],
   "source": [
    "df_breast.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert string labels to numerical labels\n",
    "# keep track of the column names of numerical labels\n",
    "label_idx_columns = [s + '_idx' for s in cpv_label_columns]\n",
    "\n",
    "# declare indexers for 3 columns\n",
    "labelIndexer = [StringIndexer(inputCol=column, outputCol=column+'_idx',handleInvalid=\"error\",\n",
    "                              stringOrderType=\"frequencyDesc\") for column in cpv_label_columns ]\n",
    "# pipeline is needed to process a list of 3 labels\n",
    "pipeline = Pipeline(stages=labelIndexer)\n",
    "# transform 3 label columns from string to number catagoies \n",
    "df_breast = pipeline.fit(df_breast).transform(df_breast)\n",
    "\n",
    "# create dictionary containing 3 label lists\n",
    "label_dict = {c.name: c.metadata[\"ml_attr\"][\"vals\"]\n",
    "for c in df.schema.fields if c.name.endswith(\"_idx\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full-feature\n",
    "full_feature_columns=['features_mirna','features_cpv']\n",
    "assembler = VectorAssembler(inputCols=full_feature_columns, outputCol='full_features')\n",
    "df_breast = assembler.transform(df_breast)\n",
    "# df = df.drop(*cpv_feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[sample_id: string, sample_type_cpv: string, disease_type_cpv: string, primary_diagnosis_cpv: string, case_id_cpv: string, features_cpv: vector, sample_type: string, disease_type: string, primary_diagnosis: string, case_id: string, features_mirna: vector, sample_type_cpv_idx: double, disease_type_cpv_idx: double, primary_diagnosis_cpv_idx: double, full_features: vector]"
     ]
    }
   ],
   "source": [
    "df_breast.select('primary_diagnosis_cpv','disease_type_cpv').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary containing 3 label lists\n",
    "label_dict = {c.name: c.metadata[\"ml_attr\"][\"vals\"]\n",
    "for c in df_breast.schema.fields if c.name.endswith(\"_idx\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[sample_id: string, sample_type_cpv: string, disease_type_cpv: string, primary_diagnosis_cpv: string, case_id_cpv: string, features_cpv: vector, sample_type: string, disease_type: string, primary_diagnosis: string, case_id: string, features_mirna: vector, sample_type_cpv_idx: double, disease_type_cpv_idx: double, primary_diagnosis_cpv_idx: double, full_features: vector]"
     ]
    }
   ],
   "source": [
    "df_breast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardization \n",
    "scaler = StandardScaler(inputCol='full_features', outputCol='scaledFeatures', withStd=True, withMean=True)\n",
    "\n",
    "# # # Convert indexed labels back to original labels\n",
    "# labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\",\n",
    "#                                labels=label_dict['disease_type_cpv_idx'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluators\n",
    "f1_evaluator=MulticlassClassificationEvaluator(predictionCol=\"prediction\",labelCol='primary_diagnosis_cpv_idx', metricName='f1')\n",
    "acc_evaluator=MulticlassClassificationEvaluator(predictionCol=\"prediction\",labelCol='primary_diagnosis_cpv_idx', metricName='accuracy')\n",
    "precision_evaluator=MulticlassClassificationEvaluator(predictionCol=\"prediction\",labelCol='primary_diagnosis_cpv_idx', metricName='weightedPrecision')\n",
    "recall_evaluator=MulticlassClassificationEvaluator(predictionCol=\"prediction\",labelCol='primary_diagnosis_cpv_idx', metricName='weightedRecall')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test/train split \n",
    "Xtest,Xtrain = df_breast.randomSplit([0.3, 0.7], seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving StandardScalar model..."
     ]
    }
   ],
   "source": [
    "s3 = boto3.client('s3')\n",
    "stdmodelPath = 'breast_std_model/data/_SUCCESS'\n",
    "if check(s3, 'gdc-emr0', stdmodelPath) == False:\n",
    "    print(\"saving StandardScalar model...\")\n",
    "    stdmodel = scaler.fit(Xtrain)\n",
    "    stdmodel.save('s3://gdc-emr0/breast_std_model')\n",
    "else:\n",
    "    from pyspark.ml.feature import StandardScalerModel\n",
    "    print(\"loading StandardScalar model...\")\n",
    "    stdmodel = StandardScalerModel.load(\"s3://gdc-emr0/breast_std_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain = stdmodel.transform(Xtrain)\n",
    "Xtest = stdmodel.transform(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to save\n",
    "# df.rdd.saveAsPickleFile(filename)\n",
    "# to load\n",
    "#pickleRdd = sc.pickleFile(filename).collect()\n",
    "# df2 = spark.createDataFrame(pickleRdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression model \n",
    "lr = LogisticRegression(aggregationDepth= 3,maxIter=1000, regParam=0.4, elasticNetParam=0.5,\n",
    "                        featuresCol='scaledFeatures',labelCol ='primary_diagnosis_cpv_idx',\n",
    "                       family='multinomial',tol=1e-06)\n",
    "\n",
    "# Hyperparameters to test\n",
    "paramGrid = ParamGridBuilder().addGrid(lr.elasticNetParam, [0.0,0.5,1.0])\\\n",
    "            .build()\n",
    "\n",
    "# K-fold cross validation \n",
    "crossval = CrossValidator(estimator=lr,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=f1_evaluator,\n",
    "                          numFolds=2,seed=seed)  # use 3+ folds in practice\n",
    "\n",
    "# Put steps in a pipeline\n",
    "pipeline = Pipeline(stages=[crossval])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "pipModel = pipeline.fit(Xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "predictions = pipModel.transform(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get hyperparameters for best model\n",
    "cvModel = pipModel.stages[-1]\n",
    "bestParams = cvModel.extractParamMap()\n",
    "# print ('Best Param (regParam): ', bestModel._java_obj.getRegParam())\n",
    "bestModel = cvModel.bestModel\n",
    "# feature_importance = bestModel.featureImportances\n",
    "# num_trees = bestModel.getNumTrees\n",
    "# tree_weights = bestModel.treeWeights\n",
    "# trees =  bestModel.trees\n",
    "# # save model\n",
    "# bestModel.save('cpv1600_rf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving Logistic Regression model..."
     ]
    }
   ],
   "source": [
    "s3 = boto3.client('s3')\n",
    "modelPath = 'breast_logistic_model/data/_SUCCESS'\n",
    "if check(s3, 'gdc-emr0', modelPath) == False:\n",
    "    print(\"saving Logistic Regression model...\")\n",
    "    bestModel.save('s3://gdc-emr0/breast_logistic_model')\n",
    "else:\n",
    "    print(modelPath+\" already exists...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score = f1_evaluator.evaluate(predictions)\n",
    "acc_score = acc_evaluator.evaluate(predictions)\n",
    "precision_score = precision_evaluator.evaluate(predictions)\n",
    "recall_score = recall_evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9111061450141634\n",
      "0.9204892966360856\n",
      "0.9192558486985007\n",
      "0.9204892966360857"
     ]
    }
   ],
   "source": [
    "print(f1_score)\n",
    "print(acc_score)\n",
    "print(precision_score)\n",
    "print(recall_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff = bestModel.coefficientMatrix\n",
    "intercepts = bestModel.interceptVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff_np = coeff.toArray()\n",
    "intercepts_np = intercepts.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_feature_columns = mirna_feature_columns+cpv_feature_columns\n",
    "coeff_df = pd.DataFrame(data=coeff_np,\n",
    "                        index=label_dict['primary_diagnosis_cpv_idx'],\n",
    "                        columns=full_feature_columns)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ETag': '\"6b0e4935bf8cd797e26e544cc7c1486d\"', 'ResponseMetadata': {'HTTPStatusCode': 200, 'RetryAttempts': 0, 'HostId': 'lIrjd39KVR6B/w38DGooXqPdGqBgFEkIaYDPch2wpjhmPgD4ELXu632k0PRLf8O81qn/iCrI2RQ=', 'HTTPHeaders': {'etag': '\"6b0e4935bf8cd797e26e544cc7c1486d\"', 'x-amz-id-2': 'lIrjd39KVR6B/w38DGooXqPdGqBgFEkIaYDPch2wpjhmPgD4ELXu632k0PRLf8O81qn/iCrI2RQ=', 'date': 'Mon, 26 Nov 2018 19:03:40 GMT', 'content-length': '0', 'x-amz-request-id': 'B2C35B7B2D7DAFA3', 'server': 'AmazonS3'}, 'RequestId': 'B2C35B7B2D7DAFA3'}}"
     ]
    }
   ],
   "source": [
    "\n",
    "from io import StringIO\n",
    "\n",
    "csv_buffer = StringIO()\n",
    "coeff_df.to_csv(csv_buffer)\n",
    "s3_resource = boto3.resource('s3')\n",
    "s3_resource.Object('gdc-emr0', 'primary_diagonisis_coeff_matrix.csv').put(Body=csv_buffer.getvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert indexed labels back to original labels.\n",
    "labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\",\n",
    "                               labels=label_dict['primary_diagnosis_cpv_idx'])\n",
    "predictions = labelConverter.transform(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_results = predictions.select('primary_diagnosis_cpv_idx','prediction','primary_diagnosis_cpv','predictedLabel','rawPrediction','probability')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[primary_diagnosis_cpv_idx: double, prediction: double, primary_diagnosis_cpv: string, predictedLabel: string, rawPrediction: vector, probability: vector]"
     ]
    }
   ],
   "source": [
    "prediction_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_results_pd = prediction_results.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ETag': '\"a7fffccef33f6e44317580b3d0009a21\"', 'ResponseMetadata': {'HTTPStatusCode': 200, 'RetryAttempts': 0, 'HostId': 'grrfP711L4XByWcovi3E2aXQFsLFwd+ctsfXnWKfe60SEfWHCsbEfKgPFB1u2KP7hSSOm0+rccQ=', 'HTTPHeaders': {'etag': '\"a7fffccef33f6e44317580b3d0009a21\"', 'x-amz-id-2': 'grrfP711L4XByWcovi3E2aXQFsLFwd+ctsfXnWKfe60SEfWHCsbEfKgPFB1u2KP7hSSOm0+rccQ=', 'date': 'Mon, 26 Nov 2018 19:22:33 GMT', 'content-length': '0', 'x-amz-request-id': 'E80D69F34AD145CE', 'server': 'AmazonS3'}, 'RequestId': 'E80D69F34AD145CE'}}"
     ]
    }
   ],
   "source": [
    "from io import StringIO\n",
    "\n",
    "csv_buffer = StringIO()\n",
    "prediction_results_pd.to_csv(csv_buffer)\n",
    "s3_resource = boto3.resource('s3')\n",
    "s3_resource.Object('gdc-emr0', 'primary_diagonisis_prediction_results.csv').put(Body=csv_buffer.getvalue())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "# Random Forest Classifier\n",
    "rf = RandomForestClassifier(cacheNodeIds=True, featuresCol='scaledFeatures',labelCol ='disease_type_idx',\\\n",
    "                           seed=seed,maxDepth=3)\n",
    "\n",
    "\n",
    "# # Hyperparameters to test\n",
    "paramGrid = ParamGridBuilder().addGrid(rf.numTrees, [200,500])\\\n",
    "            .build()\n",
    "\n",
    "# # K-fold cross validation \n",
    "crossval = CrossValidator(estimator=rf,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=f1_evaluator,\n",
    "                          numFolds=2,seed=seed)  # use 3+ folds in practice\n",
    "\n",
    "# # Put steps in a pipeline\n",
    "pipeline = Pipeline(stages=[crossval])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "pipModel = pipeline.fit(Xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "predictions = pipModel.transform(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get hyperparameters for best model\n",
    "cvModel = pipModel.stages[-1]\n",
    "bestParams = cvModel.extractParamMap()\n",
    "# print ('Best Param (regParam): ', bestModel._java_obj.getRegParam())\n",
    "bestModel = cvModel.bestModel\n",
    "feature_importance = bestModel.featureImportances\n",
    "num_trees = bestModel.getNumTrees\n",
    "tree_weights = bestModel.treeWeights\n",
    "trees =  bestModel.trees\n",
    "# # save model\n",
    "# bestModel.save('cpv1600_rf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving Random Forest model..."
     ]
    }
   ],
   "source": [
    "s3 = boto3.client('s3')\n",
    "modelPath = 'mirna_rf_model/data/_SUCCESS'\n",
    "if check(s3, 'gdc-emr0', modelPath) == False:\n",
    "    print(\"saving Random Forest model...\")\n",
    "    bestModel.save('s3://gdc-emr0/mirna_rf_model')\n",
    "else:\n",
    "    print(modelPath+\" already exists...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score = f1_evaluator.evaluate(predictions)\n",
    "acc_score = acc_evaluator.evaluate(predictions)\n",
    "precision_score = precision_evaluator.evaluate(predictions)\n",
    "recall_score = recall_evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2977658488782247\n",
      "0.3849871134020619\n",
      "0.46018467656490436\n",
      "0.3849871134020618"
     ]
    }
   ],
   "source": [
    "print(f1_score)\n",
    "print(acc_score)\n",
    "print(precision_score)\n",
    "print(recall_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert indexed labels back to original labels.\n",
    "labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\",\n",
    "                               labels=label_dict['disease_type_cpv_idx'])\n",
    "predictions = labelConverter.transform(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DataFrame constructor not properly called!\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib64/python3.4/site-packages/pandas/core/frame.py\", line 404, in __init__\n",
      "    raise ValueError('DataFrame constructor not properly called!')\n",
      "ValueError: DataFrame constructor not properly called!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_rd_df = pd.DataFrame(feature_importance, columns=['feature_importance'])\n",
    "# cpv_feature_columns+mirna_feature_columns\n",
    "from io import StringIO\n",
    "\n",
    "csv_buffer = StringIO()\n",
    "feature_rd_df.to_csv(csv_buffer)\n",
    "s3_resource = boto3.resource('s3')\n",
    "s3_resource.Object('gdc-emr0', 'rf_feature_impt.csv').put(Body=csv_buffer.getvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
