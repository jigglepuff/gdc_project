{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>1</td><td>application_1543131506044_0002</td><td>pyspark3</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-49-214.ec2.internal:20888/proxy/application_1543131506044_0002/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-60-145.ec2.internal:8042/node/containerlogs/container_1543131506044_0002_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import StringIndexer,IndexToString\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "# from pyspark.ml.feature import IDF\n",
    "# from pyspark.ml.feature import DCT\n",
    "# from pyspark.ml.feature import PolynomialExpansion\n",
    "# from pyspark.ml.feature import ChiSqSelector\n",
    "from pyspark.ml import Pipeline\n",
    "import itertools as it\n",
    "import pyspark.sql.functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 1. rename column names containing '.' (GS1-279B7.1, GS1-600G8.3 CAND1.11, HY.1)\n",
    "#      2. read csv sep = ',' for cpv final matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"DiseaseApp\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x7f1a0a291710>"
     ]
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'driverMemory': '1000M', 'executorCores': 2, 'proxyUser': 'jovyan', 'kind': 'pyspark3'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>1</td><td>application_1543131506044_0002</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-49-214.ec2.internal:20888/proxy/application_1543131506044_0002/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-60-145.ec2.internal:8042/node/containerlogs/container_1543131506044_0002_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SparkContext.setSystemProperty('spark.executor.memory', '10g')\n",
    "#sc._conf.getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to get all stored variables\n",
    "def list_dataframes():\n",
    "    from pyspark.sql import DataFrame\n",
    "    return [k for (k, v) in globals().items() if isinstance(v, DataFrame)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "from botocore.exceptions import ClientError\n",
    "\n",
    "def check(s3, bucket, key):\n",
    "    try:\n",
    "        s3.head_object(Bucket=bucket, Key=key)\n",
    "    except ClientError as e:\n",
    "        return int(e.response['Error']['Code']) != 404\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTrainingMetrics(trainingSummary,printout=True):\n",
    "    # for multiclass, we can inspect metrics on a per-label basis\n",
    "#     print(\"False positive rate by label:\")\n",
    "#     for i, rate in enumerate(trainingSummary.falsePositiveRateByLabel):\n",
    "#         print(\"label %d: %s\" % (i, rate))\n",
    "\n",
    "#     print(\"True positive rate by label:\")\n",
    "#     for i, rate in enumerate(trainingSummary.truePositiveRateByLabel):\n",
    "#         print(\"label %d: %s\" % (i, rate))\n",
    "\n",
    "#     print(\"Precision by label:\")\n",
    "#     for i, prec in enumerate(trainingSummary.precisionByLabel):\n",
    "#         print(\"label %d: %s\" % (i, prec))\n",
    "\n",
    "#     print(\"Recall by label:\")\n",
    "#     for i, rec in enumerate(trainingSummary.recallByLabel):\n",
    "#         print(\"label %d: %s\" % (i, rec))\n",
    "\n",
    "    print(\"F-measure by label:\")\n",
    "    for i, f in enumerate(trainingSummary.fMeasureByLabel()):\n",
    "        print(\"label %d: %s\" % (i, f))\n",
    "\n",
    "    accuracy = trainingSummary.accuracy\n",
    "    falsePositiveRate = trainingSummary.weightedFalsePositiveRate\n",
    "    truePositiveRate = trainingSummary.weightedTruePositiveRate\n",
    "    fMeasure = trainingSummary.weightedFMeasure()\n",
    "    precision = trainingSummary.weightedPrecision\n",
    "    recall = trainingSummary.weightedRecall\n",
    "    if printout is True:\n",
    "        print(\"Accuracy: %s\\nFPR: %s\\nTPR: %s\\nF-measure: %s\\nPrecision: %s\\nRecall: %s\"\n",
    "          % (accuracy, falsePositiveRate, truePositiveRate, fMeasure, precision, recall))\n",
    "    return {\"accuracy\": accuracy, \"fpr\": falsePositiveRate, \"tpr\": truePositiveRate, \"fmeasure\": fMeasure, \\\n",
    "            \"precision\": precision, \"recall\": recall}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data file\n",
    "mirna_path = 's3://gdc-emr0/mirna_filtered_matrix.csv'\n",
    "cpv_path = 's3://gdc-emr0/cpv_filtered_matrix.csv'\n",
    "mrna_path = 's3://gdc-emr0/mrna_filtered_matrix.csv'\n",
    "# read mirna\n",
    "df_mirna = spark.read.option(\"maxColumns\", 22400).csv(\n",
    "    mirna_path, header=True, sep = ',',mode=\"DROPMALFORMED\",inferSchema=True)\n",
    "# read cpv\n",
    "df_cpv = spark.read.option(\"maxColumns\", 22400).csv(\n",
    "    cpv_path, header=True, sep = ',',mode=\"DROPMALFORMED\",inferSchema=True)\n",
    "# # read mrna\n",
    "# df_mrna = spark.read.option(\"maxColumns\", 22400).csv(\n",
    "#     mrna_path, header=True, sep = ',',mode=\"DROPMALFORMED\",inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cpv = df_cpv.toDF(*(c.replace('.', '_') for c in df_cpv.columns))\n",
    "# df_mrna = df_mrna.toDF(*(c.replace('.', '_') for c in df_mrna.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group columns by label, identifier and feature\n",
    "label_columns = ['sample_type', 'disease_type', 'primary_diagnosis']\n",
    "mirna_identifier_columns = ['sample_id','case_id']\n",
    "mirna_feature_columns = [x for x in df_mirna.columns if x not in (label_columns+mirna_identifier_columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group columns by label, identifier and feature\n",
    "cpv_identifier_columns= ['_c0','sample_id','case_id']\n",
    "cpv_feature_columns = [x for x in df_cpv.columns if x not in (label_columns+cpv_identifier_columns)]\n",
    "df_cpv = df_cpv.withColumnRenamed(\"sample_type\", \"sample_type_cpv\").withColumnRenamed(\"disease_type\", \"disease_type_cpv\").withColumnRenamed(\"primary_diagnosis\",\"primary_diagnosis_cpv\").withColumnRenamed(\"case_id\", \"case_id_cpv\")\n",
    "cpv_label_columns = ['sample_type_cpv', 'disease_type_cpv', 'primary_diagnosis_cpv']\n",
    "cpv_identifier_columns=['_c0','sample_id','case_id_cpv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # group columns by label, identifier and feature\n",
    "# # cpv_label_columns = ['sample_type', 'disease_type', 'primary_diagnosis']\n",
    "# mrna_identifier_columns= ['_c0','sample_id','case_id']\n",
    "# mrna_feature_columns = [x for x in df_mrna.columns if x not in (label_columns+mrna_identifier_columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'Field \"hsa-let-7a-1\" does not exist.\\nAvailable fields: sample_id, sample_type, disease_type, primary_diagnosis, case_id, features_mirna'\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/ml/base.py\", line 173, in transform\n",
      "    return self._transform(dataset)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/ml/wrapper.py\", line 305, in _transform\n",
      "    return DataFrame(self._java_obj.transform(dataset._jdf), dataset.sql_ctx)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 79, in deco\n",
      "    raise IllegalArgumentException(s.split(': ', 1)[1], stackTrace)\n",
      "pyspark.sql.utils.IllegalArgumentException: 'Field \"hsa-let-7a-1\" does not exist.\\nAvailable fields: sample_id, sample_type, disease_type, primary_diagnosis, case_id, features_mirna'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# convert features into (sparse) vectors\n",
    "# mirna\n",
    "assembler = VectorAssembler(inputCols=mirna_feature_columns, outputCol='features_mirna')\n",
    "df_mirna = assembler.transform(df_mirna)\n",
    "df_mirna=df_mirna.drop(*mirna_feature_columns)\n",
    "# eventually we should store/load from HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cpv\n",
    "assembler = VectorAssembler(inputCols=cpv_feature_columns, outputCol='features_cpv')\n",
    "df_cpv = assembler.transform(df_cpv)\n",
    "df_cpv = df_cpv.drop(*cpv_feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_cpv.join(df_mirna, on=['sample_id'], how='left_outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert string labels to numerical labels\n",
    "# keep track of the column names of numerical labels\n",
    "label_idx_columns = [s + '_idx' for s in cpv_label_columns]\n",
    "\n",
    "# declare indexers for 3 columns\n",
    "labelIndexer = [StringIndexer(inputCol=column, outputCol=column+'_idx',handleInvalid=\"error\",\n",
    "                              stringOrderType=\"frequencyDesc\") for column in cpv_label_columns ]\n",
    "# pipeline is needed to process a list of 3 labels\n",
    "pipeline = Pipeline(stages=labelIndexer)\n",
    "# transform 3 label columns from string to number catagoies \n",
    "df = pipeline.fit(df).transform(df)\n",
    "\n",
    "# create dictionary containing 3 label lists\n",
    "label_dict = {c.name: c.metadata[\"ml_attr\"][\"vals\"]\n",
    "for c in df.schema.fields if c.name.endswith(\"_idx\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full-feature\n",
    "full_feature_columns=['features_mirna','features_cpv']\n",
    "assembler = VectorAssembler(inputCols=full_feature_columns, outputCol='full_features')\n",
    "df = assembler.transform(df)\n",
    "# df = df.drop(*cpv_feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardization \n",
    "scaler = StandardScaler(inputCol='full_features', outputCol='scaledFeatures', withStd=True, withMean=True)\n",
    "\n",
    "# # # Convert indexed labels back to original labels\n",
    "labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\",\n",
    "                               labels=label_dict['disease_type_cpv_idx'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluators\n",
    "f1_evaluator=MulticlassClassificationEvaluator(predictionCol=\"prediction\",labelCol='disease_type_cpv_idx', metricName='f1')\n",
    "acc_evaluator=MulticlassClassificationEvaluator(predictionCol=\"prediction\",labelCol='disease_type_cpv_idx', metricName='accuracy')\n",
    "precision_evaluator=MulticlassClassificationEvaluator(predictionCol=\"prediction\",labelCol='disease_type_cpv_idx', metricName='weightedPrecision')\n",
    "recall_evaluator=MulticlassClassificationEvaluator(predictionCol=\"prediction\",labelCol='disease_type_cpv_idx', metricName='weightedRecall')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test/train split \n",
    "Xtest,Xtrain = df.randomSplit([0.3, 0.7], seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving StandardScalar model..."
     ]
    }
   ],
   "source": [
    "s3 = boto3.client('s3')\n",
    "stdmodelPath = 'full_std_model/data/_SUCCESS'\n",
    "if check(s3, 'gdc-emr0', stdmodelPath) == False:\n",
    "    print(\"saving StandardScalar model...\")\n",
    "    stdmodel = scaler.fit(Xtrain)\n",
    "    stdmodel.save('s3://gdc-emr0/full_std_model')\n",
    "else:\n",
    "    from pyspark.ml.feature import StandardScalerModel\n",
    "    print(\"loading StandardScalar model...\")\n",
    "    stdmodel = StandardScalerModel.load(stdmodelPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain = stdmodel.transform(Xtrain)\n",
    "Xtest = stdmodel.transform(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to save\n",
    "# df.rdd.saveAsPickleFile(filename)\n",
    "# to load\n",
    "#pickleRdd = sc.pickleFile(filename).collect()\n",
    "# df2 = spark.createDataFrame(pickleRdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "# Random Forest Classifier\n",
    "rf = RandomForestClassifier(cacheNodeIds=True, featuresCol='scaledFeatures',labelCol ='disease_type_cpv_idx', numTrees=1000,\\\n",
    "                           seed=seed)\n",
    "\n",
    "\n",
    "# # Hyperparameters to test\n",
    "paramGrid = ParamGridBuilder().addGrid(rf.maxDepth, [5,7,9])\\\n",
    "            .build()\n",
    "\n",
    "# # K-fold cross validation \n",
    "crossval = CrossValidator(estimator=rf,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=f1_evaluator,\n",
    "                          numFolds=5,seed=seed)  # use 3+ folds in practice\n",
    "\n",
    "# # Put steps in a pipeline\n",
    "pipeline = Pipeline(stages=[crossval])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "pipModel = pipeline.fit(Xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "name 'pipModel' is not defined\n",
      "Traceback (most recent call last):\n",
      "NameError: name 'pipModel' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# predict\n",
    "predictions = pipModel.transform(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get hyperparameters for best model\n",
    "cvModel = pipModel.stages[-1]\n",
    "bestParams = cvModel.extractParamMap()\n",
    "# print ('Best Param (regParam): ', bestModel._java_obj.getRegParam())\n",
    "bestModel = cvModel.bestModel\n",
    "feature_importance = bestModel.featureImportances\n",
    "num_trees = bestModel.getNumTrees\n",
    "tree_weights = bestModel.treeWeights\n",
    "trees =  bestModel.trees\n",
    "# # save model\n",
    "# bestModel.save('cpv1600_rf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3')\n",
    "modelPath = 'full_rf_model/data/_SUCCESS'\n",
    "if check(s3, 'gdc-emr0', stdmodelPath) == False:\n",
    "    print(\"saving Random Forest model...\")\n",
    "    bestModel.save('s3://gdc-emr0/full_rf_model')\n",
    "else:\n",
    "    print(modelPath+\" already exists...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert indexed labels back to original labels.\n",
    "labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\",\n",
    "                               labels=label_dict['disease_type_cpv_idx'])\n",
    "predictions = labelConverter.transform(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_rd_df = pd.DataFrame(feature_importance, columns=['feature_importance'])\n",
    "# cpv_feature_columns+mirna_feature_columns\n",
    "from io import StringIO\n",
    "\n",
    "csv_buffer = StringIO()\n",
    "feature_rd_df.to_csv(csv_buffer)\n",
    "s3_resource = boto3.resource('s3')\n",
    "s3_resource.Object('gdc-emr0', 'rf_feature_impt.csv').put(Body=csv_buffer.getvalue())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
