{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>0</td><td>application_1543238914088_0001</td><td>pyspark3</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-75-139.ec2.internal:20888/proxy/application_1543238914088_0001/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-77-195.ec2.internal:8042/node/containerlogs/container_1543238914088_0001_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import StringIndexer,IndexToString\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "# from pyspark.ml.feature import IDF\n",
    "# from pyspark.ml.feature import DCT\n",
    "# from pyspark.ml.feature import PolynomialExpansion\n",
    "# from pyspark.ml.feature import ChiSqSelector\n",
    "from pyspark.ml import Pipeline\n",
    "import itertools as it\n",
    "import pyspark.sql.functions as f\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 1. rename column names containing '.' (GS1-279B7.1, GS1-600G8.3 CAND1.11, HY.1)\n",
    "#      2. read csv sep = ',' for cpv final matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"DiseaseApp\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x7f145a3bb6d8>"
     ]
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'driverMemory': '1000M', 'executorCores': 2, 'proxyUser': 'jovyan', 'kind': 'pyspark3'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>0</td><td>application_1543238914088_0001</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-75-139.ec2.internal:20888/proxy/application_1543238914088_0001/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-77-195.ec2.internal:8042/node/containerlogs/container_1543238914088_0001_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SparkContext.setSystemProperty('spark.driver.memory', '15g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "SparkContext.setSystemProperty('spark.executor.memory', '15g')\n",
    "#sc._conf.getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to get all stored variables\n",
    "def list_dataframes():\n",
    "    from pyspark.sql import DataFrame\n",
    "    return [k for (k, v) in globals().items() if isinstance(v, DataFrame)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from botocore.exceptions import ClientError\n",
    "\n",
    "def check(s3, bucket, key):\n",
    "    try:\n",
    "        s3.head_object(Bucket=bucket, Key=key)\n",
    "    except ClientError as e:\n",
    "        return int(e.response['Error']['Code']) != 404\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTrainingMetrics(trainingSummary,printout=True):\n",
    "    # for multiclass, we can inspect metrics on a per-label basis\n",
    "#     print(\"False positive rate by label:\")\n",
    "#     for i, rate in enumerate(trainingSummary.falsePositiveRateByLabel):\n",
    "#         print(\"label %d: %s\" % (i, rate))\n",
    "\n",
    "#     print(\"True positive rate by label:\")\n",
    "#     for i, rate in enumerate(trainingSummary.truePositiveRateByLabel):\n",
    "#         print(\"label %d: %s\" % (i, rate))\n",
    "\n",
    "#     print(\"Precision by label:\")\n",
    "#     for i, prec in enumerate(trainingSummary.precisionByLabel):\n",
    "#         print(\"label %d: %s\" % (i, prec))\n",
    "\n",
    "#     print(\"Recall by label:\")\n",
    "#     for i, rec in enumerate(trainingSummary.recallByLabel):\n",
    "#         print(\"label %d: %s\" % (i, rec))\n",
    "\n",
    "    print(\"F-measure by label:\")\n",
    "    for i, f in enumerate(trainingSummary.fMeasureByLabel()):\n",
    "        print(\"label %d: %s\" % (i, f))\n",
    "\n",
    "    accuracy = trainingSummary.accuracy\n",
    "    falsePositiveRate = trainingSummary.weightedFalsePositiveRate\n",
    "    truePositiveRate = trainingSummary.weightedTruePositiveRate\n",
    "    fMeasure = trainingSummary.weightedFMeasure()\n",
    "    precision = trainingSummary.weightedPrecision\n",
    "    recall = trainingSummary.weightedRecall\n",
    "    if printout is True:\n",
    "        print(\"Accuracy: %s\\nFPR: %s\\nTPR: %s\\nF-measure: %s\\nPrecision: %s\\nRecall: %s\"\n",
    "          % (accuracy, falsePositiveRate, truePositiveRate, fMeasure, precision, recall))\n",
    "    return {\"accuracy\": accuracy, \"fpr\": falsePositiveRate, \"tpr\": truePositiveRate, \"fmeasure\": fMeasure, \\\n",
    "            \"precision\": precision, \"recall\": recall}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data file\n",
    "mirna_path = 's3://gdc-emr0/mirna_filtered_matrix.csv'\n",
    "cpv_path = 's3://gdc-emr0/cpv_filtered_matrix.csv'\n",
    "mrna_path = 's3://gdc-emr0/mrna_filtered_matrix.csv'\n",
    "# read mirna\n",
    "df_mirna = spark.read.option(\"maxColumns\", 22400).csv(\n",
    "    mirna_path, header=True, sep = ',',mode=\"DROPMALFORMED\",inferSchema=True)\n",
    "# read cpv\n",
    "df_cpv = spark.read.option(\"maxColumns\", 22400).csv(\n",
    "    cpv_path, header=True, sep = ',',mode=\"DROPMALFORMED\",inferSchema=True)\n",
    "# # read mrna\n",
    "# df_mrna = spark.read.option(\"maxColumns\", 22400).csv(\n",
    "#     mrna_path, header=True, sep = ',',mode=\"DROPMALFORMED\",inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cpv = df_cpv.toDF(*(c.replace('.', '_') for c in df_cpv.columns))\n",
    "# df_mrna = df_mrna.toDF(*(c.replace('.', '_') for c in df_mrna.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group columns by label, identifier and feature\n",
    "label_columns = ['sample_type', 'disease_type', 'primary_diagnosis']\n",
    "mirna_identifier_columns = ['sample_id','case_id']\n",
    "mirna_feature_columns = [x for x in df_mirna.columns if x not in (label_columns+mirna_identifier_columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group columns by label, identifier and feature\n",
    "cpv_identifier_columns= ['_c0','sample_id','case_id']\n",
    "cpv_feature_columns = [x for x in df_cpv.columns if x not in (label_columns+cpv_identifier_columns)]\n",
    "df_cpv = df_cpv.withColumnRenamed(\"sample_type\", \"sample_type_cpv\").withColumnRenamed(\"disease_type\", \"disease_type_cpv\").withColumnRenamed(\"primary_diagnosis\",\"primary_diagnosis_cpv\").withColumnRenamed(\"case_id\", \"case_id_cpv\")\n",
    "cpv_label_columns = ['sample_type_cpv', 'disease_type_cpv', 'primary_diagnosis_cpv']\n",
    "cpv_identifier_columns=['_c0','sample_id','case_id_cpv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # group columns by label, identifier and feature\n",
    "# # cpv_label_columns = ['sample_type', 'disease_type', 'primary_diagnosis']\n",
    "# mrna_identifier_columns= ['_c0','sample_id','case_id']\n",
    "# mrna_feature_columns = [x for x in df_mrna.columns if x not in (label_columns+mrna_identifier_columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert features into (sparse) vectors\n",
    "# mirna\n",
    "assembler = VectorAssembler(inputCols=mirna_feature_columns, outputCol='features_mirna')\n",
    "df_mirna = assembler.transform(df_mirna)\n",
    "df_mirna=df_mirna.drop(*mirna_feature_columns)\n",
    "# eventually we should store/load from HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cpv\n",
    "assembler = VectorAssembler(inputCols=cpv_feature_columns, outputCol='features_cpv')\n",
    "df_cpv = assembler.transform(df_cpv)\n",
    "df_cpv = df_cpv.drop(*cpv_feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_cpv.join(df_mirna, on=['sample_id'], how='left_outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert string labels to numerical labels\n",
    "# keep track of the column names of numerical labels\n",
    "label_idx_columns = [s + '_idx' for s in cpv_label_columns]\n",
    "\n",
    "# declare indexers for 3 columns\n",
    "labelIndexer = [StringIndexer(inputCol=column, outputCol=column+'_idx',handleInvalid=\"error\",\n",
    "                              stringOrderType=\"frequencyDesc\") for column in cpv_label_columns ]\n",
    "# pipeline is needed to process a list of 3 labels\n",
    "pipeline = Pipeline(stages=labelIndexer)\n",
    "# transform 3 label columns from string to number catagoies \n",
    "df = pipeline.fit(df).transform(df)\n",
    "\n",
    "# create dictionary containing 3 label lists\n",
    "label_dict = {c.name: c.metadata[\"ml_attr\"][\"vals\"]\n",
    "for c in df.schema.fields if c.name.endswith(\"_idx\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full-feature\n",
    "full_feature_columns=['features_mirna','features_cpv']\n",
    "assembler = VectorAssembler(inputCols=full_feature_columns, outputCol='full_features')\n",
    "df = assembler.transform(df)\n",
    "# df = df.drop(*cpv_feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardization \n",
    "scaler = StandardScaler(inputCol='full_features', outputCol='scaledFeatures', withStd=True, withMean=True)\n",
    "\n",
    "# # # Convert indexed labels back to original labels\n",
    "labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\",\n",
    "                               labels=label_dict['disease_type_cpv_idx'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluators\n",
    "f1_evaluator=MulticlassClassificationEvaluator(predictionCol=\"prediction\",labelCol='disease_type_cpv_idx', metricName='f1')\n",
    "acc_evaluator=MulticlassClassificationEvaluator(predictionCol=\"prediction\",labelCol='disease_type_cpv_idx', metricName='accuracy')\n",
    "precision_evaluator=MulticlassClassificationEvaluator(predictionCol=\"prediction\",labelCol='disease_type_cpv_idx', metricName='weightedPrecision')\n",
    "recall_evaluator=MulticlassClassificationEvaluator(predictionCol=\"prediction\",labelCol='disease_type_cpv_idx', metricName='weightedRecall')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test/train split \n",
    "Xtest,Xtrain = df.randomSplit([0.3, 0.7], seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading StandardScalar model..."
     ]
    }
   ],
   "source": [
    "s3 = boto3.client('s3')\n",
    "stdmodelPath = 'full_std_model/data/_SUCCESS'\n",
    "if check(s3, 'gdc-emr0', stdmodelPath) == False:\n",
    "    print(\"saving StandardScalar model...\")\n",
    "#     stdmodel = scaler.fit(Xtrain)\n",
    "    stdmodel.save('s3://gdc-emr0/full_std_model')\n",
    "else:\n",
    "    from pyspark.ml.feature import StandardScalerModel\n",
    "    print(\"loading StandardScalar model...\")\n",
    "    stdmodel = StandardScalerModel.load(\"s3://gdc-emr0/full_std_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain = stdmodel.transform(Xtrain)\n",
    "Xtest = stdmodel.transform(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving IDF model..."
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import IDF\n",
    "idf = IDF(inputCol=\"scaledFeatures\", outputCol=\"idffeatures\")\n",
    "s3 = boto3.client('s3')\n",
    "idfmodelPath = 'full_idf_model/data/_SUCCESS'\n",
    "if check(s3, 'gdc-emr0', idfmodelPath) == False:\n",
    "    print(\"saving IDF model...\")\n",
    "    idfModel = idf.fit(Xtrain)\n",
    "    idfModel.save('s3://gdc-emr0/full_idf_model')\n",
    "else:\n",
    "    from pyspark.ml.feature import IDFModel\n",
    "    print(\"loading IDF model...\")\n",
    "    idfModel = IDFModel.load(\"s3://gdc-emr0/full_idf_model\")\n",
    "Xtrain = idfModel.transform(Xtrain)\n",
    "Xtest = idfModel.transform(Xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression model \n",
    "lr = LogisticRegression(aggregationDepth= 3,maxIter=1000, regParam=0.4, elasticNetParam=0.5,\n",
    "                        featuresCol='idffeatures',labelCol ='disease_type_cpv_idx',\n",
    "                       family='multinomial',tol=1e-06)\n",
    "\n",
    "# Hyperparameters to test\n",
    "paramGrid = ParamGridBuilder().addGrid(lr.elasticNetParam, [0.0,0.5,1.0])\\\n",
    "                            .addGrid(lr.regParam,[0.1,0.5,1.0])\\\n",
    "                            .build()\n",
    "\n",
    "# K-fold cross validation \n",
    "crossval = CrossValidator(estimator=lr,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=f1_evaluator,\n",
    "                          numFolds=3,seed=seed)  # use 3+ folds in practice\n",
    "\n",
    "# Put steps in a pipeline\n",
    "pipeline = Pipeline(stages=[crossval])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "pipModel = pipeline.fit(Xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "predictions = pipModel.transform(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get hyperparameters for best model\n",
    "cvModel = pipModel.stages[-1]\n",
    "bestParams = cvModel.extractParamMap()\n",
    "# print ('Best Param (regParam): ', bestModel._java_obj.getRegParam())\n",
    "bestModel = cvModel.bestModel\n",
    "# feature_importance = bestModel.featureImportances\n",
    "# num_trees = bestModel.getNumTrees\n",
    "# tree_weights = bestModel.treeWeights\n",
    "# trees =  bestModel.trees\n",
    "# # save model\n",
    "# bestModel.save('cpv1600_rf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3')\n",
    "modelPath = 'full_std_idf_logistic_model/data/_SUCCESS'\n",
    "if check(s3, 'gdc-emr0', modelPath) == False:\n",
    "    print(\"saving std-idf Logistic Regression model...\")\n",
    "    bestModel.save('s3://gdc-emr0/full_std_idf_logistic_model')\n",
    "else:\n",
    "    print(modelPath+\" already exists...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score = f1_evaluator.evaluate(predictions)\n",
    "acc_score = acc_evaluator.evaluate(predictions)\n",
    "precision_score = precision_evaluator.evaluate(predictions)\n",
    "recall_score = recall_evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f1_score)\n",
    "print(acc_score)\n",
    "print(precision_score)\n",
    "print(recall_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff = bestModel.coefficientMatrix\n",
    "intercepts = bestModel.interceptVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff_np = coeff.toArray()\n",
    "intercepts_np = intercepts.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_feature_columns = mirna_feature_columns+cpv_feature_columns\n",
    "coeff_df = pd.DataFrame(data=coeff_np,\n",
    "                        index=label_dict['disease_type_cpv_idx'],\n",
    "                        columns=full_feature_columns)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff_df['intercept'] = pd.Series(intercepts_np, index=coeff_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ETag': '\"fa49bb0788163cadb8640fe560a4afd4\"', 'ResponseMetadata': {'HTTPStatusCode': 200, 'RetryAttempts': 0, 'HostId': 'cWXl2nJRH29xqUvxQ4CC51Csjnlfa8XbEI9t4PYCSjBTgAnIZgVcZ8NJ4aFpCq36nZ+J7Kpw9jU=', 'HTTPHeaders': {'etag': '\"fa49bb0788163cadb8640fe560a4afd4\"', 'x-amz-id-2': 'cWXl2nJRH29xqUvxQ4CC51Csjnlfa8XbEI9t4PYCSjBTgAnIZgVcZ8NJ4aFpCq36nZ+J7Kpw9jU=', 'date': 'Mon, 26 Nov 2018 09:43:16 GMT', 'content-length': '0', 'x-amz-request-id': '4C96E9183A9FB755', 'server': 'AmazonS3'}, 'RequestId': '4C96E9183A9FB755'}}"
     ]
    }
   ],
   "source": [
    "from io import StringIO\n",
    "import boto3\n",
    "\n",
    "csv_buffer = StringIO()\n",
    "coeff_df.to_csv(csv_buffer)\n",
    "s3_resource = boto3.resource('s3')\n",
    "s3_resource.Object(\"gdc-emr0\", 'full_logistic_coeff.csv').put(Body=csv_buffer.getvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.join(df1,predictions.col(\"predictedLabel\")!=predictions.col(\"disease_type_cpv\"),\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_scores1['is_score_chased'] = np.where(predictions['predictedLabel']!=predictions['disease_type_cpv'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to save\n",
    "# df.rdd.saveAsPickleFile(filename)\n",
    "# to load\n",
    "#pickleRdd = sc.pickleFile(filename).collect()\n",
    "# df2 = spark.createDataFrame(pickleRdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "# Random Forest Classifier\n",
    "rf = RandomForestClassifier(cacheNodeIds=True, featuresCol='scaledFeatures',labelCol ='disease_type_cpv_idx',\\\n",
    "                           seed=seed,maxDepth=3)\n",
    "\n",
    "\n",
    "# # Hyperparameters to test\n",
    "paramGrid = ParamGridBuilder().addGrid(rf.numTrees, [200,500])\\\n",
    "            .build()\n",
    "\n",
    "# # K-fold cross validation \n",
    "crossval = CrossValidator(estimator=rf,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=f1_evaluator,\n",
    "                          numFolds=2,seed=seed)  # use 3+ folds in practice\n",
    "\n",
    "# # Put steps in a pipeline\n",
    "pipeline = Pipeline(stages=[crossval])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "pipModel = pipeline.fit(Xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "predictions = pipModel.transform(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get hyperparameters for best model\n",
    "cvModel = pipModel.stages[-1]\n",
    "bestParams = cvModel.extractParamMap()\n",
    "# print ('Best Param (regParam): ', bestModel._java_obj.getRegParam())\n",
    "bestModel = cvModel.bestModel\n",
    "feature_importance = bestModel.featureImportances\n",
    "num_trees = bestModel.getNumTrees\n",
    "tree_weights = bestModel.treeWeights\n",
    "trees =  bestModel.trees\n",
    "# # save model\n",
    "# bestModel.save('cpv1600_rf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving Random Forest model..."
     ]
    }
   ],
   "source": [
    "s3 = boto3.client('s3')\n",
    "modelPath = 'full_rf_model/data/_SUCCESS'\n",
    "if check(s3, 'gdc-emr0', modelPath) == False:\n",
    "    print(\"saving Random Forest model...\")\n",
    "    bestModel.save('s3://gdc-emr0/full_rf_model')\n",
    "else:\n",
    "    print(modelPath+\" already exists...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score = f1_evaluator.evaluate(predictions)\n",
    "acc_score = acc_evaluator.evaluate(predictions)\n",
    "precision_score = precision_evaluator.evaluate(predictions)\n",
    "recall_score = recall_evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.31670977514005116\n",
      "0.38067254325824357\n",
      "0.48265841425755696\n",
      "0.3806725432582435"
     ]
    }
   ],
   "source": [
    "print(f1_score)\n",
    "print(acc_score)\n",
    "print(precision_score)\n",
    "print(recall_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert indexed labels back to original labels.\n",
    "labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\",\n",
    "                               labels=label_dict['disease_type_cpv_idx'])\n",
    "predictions = labelConverter.transform(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelConverter.save('s3://gdc-emr0/disease_type_full_index2string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|      predictedLabel|    disease_type_cpv|\n",
      "+--------------------+--------------------+\n",
      "|              Normal|Kidney Renal Papi...|\n",
      "|Colon Adenocarcinoma|Rectum Adenocarci...|\n",
      "|Pancreatic Adenoc...|        Mesothelioma|\n",
      "|Uterine Corpus En...|Uterine Corpus En...|\n",
      "|Lung Squamous Cel...|Lung Squamous Cel...|\n",
      "|Head and Neck Squ...|Head and Neck Squ...|\n",
      "|Testicular Germ C...|Testicular Germ C...|\n",
      "|Ovarian Serous Cy...|Ovarian Serous Cy...|\n",
      "|Breast Invasive C...|Breast Invasive C...|\n",
      "|Kidney Renal Clea...|Kidney Renal Clea...|\n",
      "|Breast Invasive C...|Breast Invasive C...|\n",
      "|Lung Squamous Cel...|Lung Squamous Cel...|\n",
      "|Breast Invasive C...|Lung Squamous Cel...|\n",
      "|Kidney Renal Clea...|Kidney Renal Clea...|\n",
      "|   Thyroid Carcinoma|   Thyroid Carcinoma|\n",
      "|Uterine Corpus En...|Cervical Squamous...|\n",
      "|Lung Squamous Cel...|Lung Squamous Cel...|\n",
      "|Bladder Urothelia...|Bladder Urothelia...|\n",
      "|             Sarcoma|             Sarcoma|\n",
      "|Head and Neck Squ...|Bladder Urothelia...|\n",
      "|Breast Invasive C...|Breast Invasive C...|\n",
      "|Rectum Adenocarci...|Rectum Adenocarci...|\n",
      "|Breast Invasive C...|Breast Invasive C...|\n",
      "|Cervical Squamous...|Cervical Squamous...|\n",
      "|Acute Myeloid Leu...|Acute Myeloid Leu...|\n",
      "|Bladder Urothelia...|Kidney Renal Papi...|\n",
      "|Breast Invasive C...|Colon Adenocarcinoma|\n",
      "|             Sarcoma|             Sarcoma|\n",
      "|Kidney Renal Clea...|             Sarcoma|\n",
      "|Bladder Urothelia...|Bladder Urothelia...|\n",
      "|Kidney Renal Clea...|Kidney Renal Clea...|\n",
      "|Skin Cutaneous Me...|      Uveal Melanoma|\n",
      "|Prostate Adenocar...|Prostate Adenocar...|\n",
      "|Head and Neck Squ...|Head and Neck Squ...|\n",
      "|Uterine Corpus En...|Uterine Corpus En...|\n",
      "|Lung Squamous Cel...|Breast Invasive C...|\n",
      "|Lung Squamous Cel...|Lung Squamous Cel...|\n",
      "|Colon Adenocarcinoma|Rectum Adenocarci...|\n",
      "|Breast Invasive C...|Breast Invasive C...|\n",
      "|              Normal|              Normal|\n",
      "|   Thyroid Carcinoma|   Thyroid Carcinoma|\n",
      "|Kidney Renal Papi...|Kidney Renal Papi...|\n",
      "|Colon Adenocarcinoma|Rectum Adenocarci...|\n",
      "|             Thymoma|             Thymoma|\n",
      "|              Normal|              Normal|\n",
      "|Breast Invasive C...|Breast Invasive C...|\n",
      "|Uterine Corpus En...|Uterine Corpus En...|\n",
      "|Pancreatic Adenoc...|Lung Squamous Cel...|\n",
      "|Breast Invasive C...|Stomach Adenocarc...|\n",
      "|Kidney Renal Clea...|Kidney Renal Clea...|\n",
      "|Colon Adenocarcinoma|Rectum Adenocarci...|\n",
      "|Lymphoid Neoplasm...|Lymphoid Neoplasm...|\n",
      "|              Normal|Liver Hepatocellu...|\n",
      "|              Normal|              Normal|\n",
      "|Colon Adenocarcinoma|Colon Adenocarcinoma|\n",
      "|Ovarian Serous Cy...|Ovarian Serous Cy...|\n",
      "|Cervical Squamous...|Cervical Squamous...|\n",
      "|Breast Invasive C...|Lung Squamous Cel...|\n",
      "|Pancreatic Adenoc...|Pancreatic Adenoc...|\n",
      "|Breast Invasive C...|Cervical Squamous...|\n",
      "|Breast Invasive C...|Breast Invasive C...|\n",
      "|Brain Lower Grade...|Brain Lower Grade...|\n",
      "|Colon Adenocarcinoma|Colon Adenocarcinoma|\n",
      "|Prostate Adenocar...|Prostate Adenocar...|\n",
      "|              Normal|              Normal|\n",
      "|Colon Adenocarcinoma|Colon Adenocarcinoma|\n",
      "|              Normal|Adrenocortical Ca...|\n",
      "|Kidney Renal Papi...|Kidney Renal Papi...|\n",
      "|Head and Neck Squ...|Head and Neck Squ...|\n",
      "| Lung Adenocarcinoma| Lung Adenocarcinoma|\n",
      "|Kidney Renal Clea...|Kidney Renal Clea...|\n",
      "|Lung Squamous Cel...|Stomach Adenocarc...|\n",
      "|Kidney Renal Clea...|Kidney Renal Clea...|\n",
      "|             Sarcoma|             Sarcoma|\n",
      "|Colon Adenocarcinoma|Cervical Squamous...|\n",
      "|Uterine Corpus En...|Uterine Carcinosa...|\n",
      "|Prostate Adenocar...|Prostate Adenocar...|\n",
      "|             Sarcoma|             Sarcoma|\n",
      "|Skin Cutaneous Me...|Skin Cutaneous Me...|\n",
      "|Breast Invasive C...|Breast Invasive C...|\n",
      "|              Normal|Breast Invasive C...|\n",
      "| Lung Adenocarcinoma| Lung Adenocarcinoma|\n",
      "|Brain Lower Grade...|Brain Lower Grade...|\n",
      "|Colon Adenocarcinoma|Colon Adenocarcinoma|\n",
      "|Cervical Squamous...|Cervical Squamous...|\n",
      "|Brain Lower Grade...|Brain Lower Grade...|\n",
      "|Prostate Adenocar...|Prostate Adenocar...|\n",
      "|Skin Cutaneous Me...|Skin Cutaneous Me...|\n",
      "|Head and Neck Squ...|              Normal|\n",
      "|Kidney Renal Clea...|Kidney Renal Clea...|\n",
      "|Kidney Renal Clea...|Lung Squamous Cel...|\n",
      "|Brain Lower Grade...|Brain Lower Grade...|\n",
      "|Colon Adenocarcinoma|Colon Adenocarcinoma|\n",
      "|Uterine Corpus En...|Uterine Corpus En...|\n",
      "|Pheochromocytoma ...|Pheochromocytoma ...|\n",
      "|Colon Adenocarcinoma|Colon Adenocarcinoma|\n",
      "| Lung Adenocarcinoma| Lung Adenocarcinoma|\n",
      "|Brain Lower Grade...|Brain Lower Grade...|\n",
      "|Skin Cutaneous Me...|Skin Cutaneous Me...|\n",
      "|Kidney Renal Papi...|Kidney Renal Papi...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 100 rows"
     ]
    }
   ],
   "source": [
    "predictions.select('predictedLabel','disease_type_cpv').show(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DataFrame constructor not properly called!\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib64/python3.4/site-packages/pandas/core/frame.py\", line 404, in __init__\n",
      "    raise ValueError('DataFrame constructor not properly called!')\n",
      "ValueError: DataFrame constructor not properly called!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_rd_df = pd.DataFrame(feature_importance, columns=['feature_importance'])\n",
    "# cpv_feature_columns+mirna_feature_columns\n",
    "from io import StringIO\n",
    "\n",
    "csv_buffer = StringIO()\n",
    "feature_rd_df.to_csv(csv_buffer)\n",
    "s3_resource = boto3.resource('s3')\n",
    "s3_resource.Object('gdc-emr0', 'rf_feature_impt.csv').put(Body=csv_buffer.getvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark3",
   "language": "",
   "name": "pyspark3kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark3",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
