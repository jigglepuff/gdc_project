{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mount Elastic NFS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘efs’: File exists\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "mkdir efs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh \n",
    "\n",
    "sudo mount -t nfs \\\n",
    "    -o nfsvers=4.1,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2 \\\n",
    "    172.31.14.15:/ \\\n",
    "    ./efs\n",
    "\n",
    "sudo chmod go+rw ./efs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "from sagemaker import get_execution_role\n",
    "import random\n",
    "import os\n",
    "import hashlib\n",
    "import sys\n",
    "import json \n",
    "import requests\n",
    "\n",
    "# from utils import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "us-east-1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'0.22.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_region = boto3.session.Session().region_name # set the region of the instance\n",
    "print(my_region)\n",
    "# boto3.describe_mount_targets(FileSystemId='<file_system_id>')\n",
    "pd.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate caseid_fileid matrix (Copy Number Variation & miRNA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "read the json file and parse the file id and case id info and save it \n",
    "'''\n",
    "# CPV file\n",
    "# json_file = \"/efs/manifests/files.2018-11-07-cpv.json\"\n",
    "# outputfile = \"file_caseid_cpv.csv\"\n",
    "\n",
    "# miRNA file\n",
    "# json_file = \"/efs/manifests/metadata-miRNA.json\"\n",
    "# outputfile = \"file_caseid_miRna.csv\"\n",
    "\n",
    "# DNA Methylation file\n",
    "# json_file = \"efs/manifests/files.2018-11-08-dna_meth.json\"\n",
    "# outputfile = \"file_caseid_dnaMeth.csv\"\n",
    "\n",
    "# mRNA file\n",
    "json_file = \"efs/manifests/files.2018-11-08-mrna.json\"\n",
    "outputfile = \"file_caseid_mrna.csv\"\n",
    "\n",
    "\n",
    "with open(json_file) as data_file:    \n",
    "    data = json.load(data_file)\n",
    "    \n",
    "data_arr = []\n",
    "case_ids = set()\n",
    "for each_record in data:\n",
    "    # print (each_record)\n",
    "    file_id = each_record['file_id']\n",
    "    case_id =  each_record['cases'][0]['case_id']\n",
    "    if case_id in case_ids:\n",
    "        case_ids.add(case_id)\n",
    "\n",
    "    else:\n",
    "\n",
    "        data_arr.append([file_id,case_id])\n",
    "\n",
    "df = pd.DataFrame(data_arr, columns = ['file_id','case_id'])\n",
    "\n",
    "df.to_csv(outputfile,index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Request Metadata (Copy Num Variation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34713\n",
      "12586982\n"
     ]
    }
   ],
   "source": [
    "# for CPV\n",
    "# metafile = \"files_meta_cpv.csv\"\n",
    "# for miRNA\n",
    "# metafile = \"files_meta_miRNA.csv\"\n",
    "# for DNAmeth\n",
    "# metafile = \"files_meta_dnaMeth.csv\"\n",
    "# for RNA \n",
    "metafile = \"files_meta_mRNA.csv\"\n",
    "\n",
    "df = pd.read_csv(outputfile)\n",
    "case_ids = df.case_id.values\n",
    "file_ids = df.file_id.values\n",
    "fd = open(metafile,'w')\n",
    "size = file_ids.shape[0] + 1000\n",
    "id_list = file_ids.tolist()\n",
    "print(len(id_list))\n",
    "# print(len(id_list[:14713]))\n",
    "\n",
    "cases_endpt = 'https://api.gdc.cancer.gov/files'\n",
    "fields = [\n",
    "    \"file_id\",\n",
    "    \"file_name\",\n",
    "#     \"cases.submitter_id\",\n",
    "    \"cases.case_id\",\n",
    "    \"data_category\",\n",
    "    \"data_type\",\n",
    "    # \"cases.samples.tumor_descriptor\",\n",
    "    \"cases.samples.sample_type\",\n",
    "    \"cases.project.disease_type\",\n",
    "    \"cases.diagnoses.primary_diagnosis\",\n",
    "    # \"cases.project.name\",\n",
    "#     \"cases.project.project_id\",\n",
    "#     \"cases.samples.sample_type\",\n",
    "#     \"cases.samples.submitter_id\",\n",
    "    \"cases.samples.sample_id\",\n",
    "    \"cases.samples.portions.analytes.aliquots.aliquot_id\",\n",
    "#     \"cases.diagnoses.classification_of_tumor\",\n",
    "#     \"cases.samples.portions.slides.percent_tumor_cells\",\n",
    "#     \"cases.samples.portions.slides.percent_normal_cells\",\n",
    "#     \"cases.samples.portions.slides.percent_stromal_cells\"\n",
    "    # \"cases.samples.portions.analytes.aliquots.submitter_id\"\n",
    "    ]\n",
    "\n",
    "\n",
    "filters = {\n",
    "    \"op\":\"in\",\n",
    "    \"content\":{\n",
    "        \"field\":\"files.file_id\",\n",
    "#         \"value\": file_ids.tolist()\n",
    "        \"value\":id_list[:22200]\n",
    "    }\n",
    "}\n",
    "filters2 = {\n",
    "    \"op\":\"in\",\n",
    "    \"content\":{\n",
    "        \"field\":\"files.file_id\",\n",
    "#         \"value\": file_ids.tolist()\n",
    "        \"value\":id_list[22200:]\n",
    "    }\n",
    "}\n",
    "\n",
    "# print (filters)\n",
    "fields = ','.join(fields)\n",
    "#expand group is diagnosis and demoragphic\n",
    "params = {\n",
    "    \"filters\" : filters,\n",
    "    \"fields\": fields,\n",
    "    # \"expand\" : \"diagnoses,demographic,exposures\",\n",
    "    \"format\": \"CSV\",\n",
    "    \"pretty\": \"true\",\n",
    "    \"size\": size\n",
    "}\n",
    "\n",
    "params2 = {\n",
    "    \"filters\" : filters2,\n",
    "    \"fields\": fields,\n",
    "    # \"expand\" : \"diagnoses,demographic,exposures\",\n",
    "    \"format\": \"CSV\",\n",
    "    \"pretty\": \"true\",\n",
    "    \"size\": size\n",
    "}\n",
    "# print (params)\n",
    "#print (filters)\n",
    "#print (fields)\n",
    "\n",
    "\n",
    "response = requests.post(cases_endpt, headers = {\"Content-Type\": \"application/json\"},json = params)\n",
    "fd.write(response.content.decode(\"utf-8\"))\n",
    "response2 = requests.post(cases_endpt, headers = {\"Content-Type\": \"application/json\"},json = params2)\n",
    "fd.write(response2.content.decode(\"utf-8\"))\n",
    "fd.close()\n",
    "\n",
    "# edit\n",
    "meta_RNA = pd.read_csv(metafile)\n",
    "meta_RNA=meta_RNA.drop(meta_RNA.index[22200])\n",
    "meta_RNA=meta_RNA.dropna(how='any', axis=1,thresh=1000)\n",
    "meta_RNA.to_csv(metafile,index=False)\n",
    "\n",
    "filestat = os.stat(metafile)\n",
    "print (filestat.st_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of CPV samples:  22142\n",
      "# of miRNA samples:  11422\n",
      "# of DNA_Meth samples:  12172\n",
      "# of RNA samples:  11535\n",
      "# of unique samples: 23166\n",
      "# of unique samples with complete info: 10133\n"
     ]
    }
   ],
   "source": [
    "# Combine to form file and label matrix\n",
    "from collections import defaultdict\n",
    "meta_cpv = pd.read_csv(\"files_meta_cpv.csv\")\n",
    "meta_miRNA = pd.read_csv(\"files_meta_miRNA.csv\")\n",
    "meta_dnaMeth = pd.read_csv(\"files_meta_dnaMeth.csv\")\n",
    "meta_mRNA = pd.read_csv(\"files_meta_mRNA.csv\")\n",
    "\n",
    "\n",
    "cpv_file_dict = dict(zip(meta_cpv['cases.0.samples.0.sample_id'], meta_cpv['file_id']))\n",
    "miRNA_file_dict = dict(zip(meta_miRNA['cases.0.samples.0.sample_id'], meta_miRNA['file_id']))\n",
    "dnaMeth_file_dict = dict(zip(meta_dnaMeth['cases.0.samples.0.sample_id'], meta_dnaMeth['file_id']))\n",
    "mRNA_file_dict = dict(zip(meta_mRNA['cases.0.samples.0.sample_id'], meta_mRNA['file_id']))\n",
    "\n",
    "print(\"# of CPV samples: \",len(cpv_file_dict))\n",
    "print(\"# of miRNA samples: \",len(miRNA_file_dict))\n",
    "print(\"# of DNA_Meth samples: \",len(dnaMeth_file_dict))\n",
    "print(\"# of RNA samples: \",len(RNA_file_dict))\n",
    "\n",
    "dicts = [cpv_file_dict,\n",
    "         miRNA_file_dict,\n",
    "        dnaMeth_file_dict,\n",
    "        mRNA_file_dict]\n",
    "super_file_dict = defaultdict(set)  # uses set to avoid duplicates\n",
    "for d in dicts:\n",
    "    for k, v in d.items(): \n",
    "        super_file_dict[k].add(v)\n",
    "\n",
    "        \n",
    "print(\"# of unique samples:\",len(super_file_dict))\n",
    "\n",
    "# temp = pd.DataFrame.from_dict(super_file_dict, orient='index')\n",
    "# print(temp.shape)\n",
    "# temp.columns=[\"pv\",\"mirna\",\"dna_meth\",\"mrna\"]\n",
    "# temp.index.name=\"sample_id\"\n",
    "# temp.to_csv(\"temp.csv\",index=True)\n",
    "\n",
    "# delete entries without both entries \n",
    "super_file_dict = { k:v for k,v in super_file_dict.items() if len(v)==len(dicts) }\n",
    "print(\"# of unique samples with complete info:\",len(super_file_dict))     \n",
    "# print(dict(list(super_dict.items())[0:3]))\n",
    "# print(meta_cpv.shape[0])\n",
    "# print(len(super_dict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:2: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "  from ipykernel import kernelapp as app\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:4: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:6: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:8: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10133\n"
     ]
    }
   ],
   "source": [
    "cpv_label_df = meta_cpv[['cases.0.samples.0.sample_id','cases.0.samples.0.sample_type','cases.0.project.disease_type','cases.0.diagnoses.0.primary_diagnosis','cases.0.case_id']]\n",
    "cpv_label_dict = cpv_label_df.set_index('cases.0.samples.0.sample_id').T.to_dict('list')\n",
    "miRNA_label_df = meta_miRNA[['cases.0.samples.0.sample_id','cases.0.samples.0.sample_type','cases.0.project.disease_type','cases.0.diagnoses.0.primary_diagnosis','cases.0.case_id']]\n",
    "miRNA_label_dict = miRNA_label_df.set_index('cases.0.samples.0.sample_id').T.to_dict('list')\n",
    "DNA_meth_label_df = meta_dnaMeth[['cases.0.samples.0.sample_id','cases.0.samples.0.sample_type','cases.0.project.disease_type','cases.0.diagnoses.0.primary_diagnosis','cases.0.case_id']]\n",
    "DNA_meth_label_dict = DNA_meth_label_df.set_index('cases.0.samples.0.sample_id').T.to_dict('list')\n",
    "mRNA_label_df = meta_mRNA[['cases.0.samples.0.sample_id','cases.0.samples.0.sample_type','cases.0.project.disease_type','cases.0.diagnoses.0.primary_diagnosis','cases.0.case_id']]\n",
    "mRNA_label_dict = mRNA_label_df.set_index('cases.0.samples.0.sample_id').T.to_dict('list')\n",
    "# print(miRNA_file_dict['22be9844-4d9e-4372-a46a-b5c64480e5aa'])\n",
    "# print(cpv_file_dict['22be9844-4d9e-4372-a46a-b5c64480e5aa'])\n",
    "# print(\"mRNA entries\", len(miRNA_label_dict))\n",
    "# print(\"CPV entries\",len(cpv_label_dict))\n",
    "\n",
    "\n",
    "dicts = [cpv_label_dict,\n",
    "         miRNA_label_dict,\n",
    "        DNA_meth_dict,\n",
    "        mRNA_label_dict]\n",
    "super_meta_dict = defaultdict(set)  # uses set to avoid duplicates\n",
    "for d in dicts:\n",
    "    for k, v in d.items(): \n",
    "        super_meta_dict[k] = v\n",
    "\n",
    "# # delete entries without both entries \n",
    "for key in list(super_meta_dict):\n",
    "    if key not in super_file_dict.keys():\n",
    "        super_meta_dict.pop(key)\n",
    "        \n",
    "    \n",
    "# Check if the same size\n",
    "# print(len(super_meta_dict))\n",
    "print(len(super_file_dict))\n",
    "# print(dict(list(super_meta_dict.items())[0:3]))\n",
    "# print(dict(list(super_file_dict.items())[0:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10718\n",
      "10718\n",
      "(10718, 2)\n",
      "(10718, 1937)\n"
     ]
    }
   ],
   "source": [
    "# Generate combined CPV & miRNA matrix\n",
    "print(len(super_file_dict))\n",
    "print(len(super_meta_dict))\n",
    "\n",
    "cpv_features = pd.read_csv(\"cpv.csv\")\n",
    "miRna_features = pd.read_csv(\"miRNA.csv\")\n",
    "df2 = pd.DataFrame.from_dict(super_meta_dict, orient='index')\n",
    "df2.columns=[\"sample_type\",\"disease_type\",\"primary_diagnosis\",\"case_id\"]\n",
    "df2.index.name=\"sample_id\"\n",
    "\n",
    "df = pd.DataFrame.from_dict(super_file_dict, orient='index')\n",
    "print(df.shape)\n",
    "df.columns=[\"file_id_cpv\",\"file_id\"]\n",
    "df.index.name=\"sample_id\"\n",
    "df = pd.concat([df.reset_index(), miRna_features], axis=1)\n",
    "df.set_index('sample_id')\n",
    "df.pop('file_id')\n",
    "df.pop('disease_type')\n",
    "df.rename(columns={'file_id_cpv':'file_id'}, inplace=True)\n",
    "df = pd.concat([df.reset_index(), cpv_features], axis=1)\n",
    "df.set_index('sample_id')\n",
    "df.pop('file_id')\n",
    "\n",
    "# df=df.reset_index().merge(df2,how = 'right',on=\"sample_id\").set_index('sample_id')\n",
    "df = pd.concat([df.reset_index(), df2.reset_index()], axis=1)\n",
    "df.set_index('sample_id')\n",
    "\n",
    "df=df.dropna(axis=0,how='any')\n",
    "\n",
    "print(df.shape)\n",
    "\n",
    "df.to_csv(\"final_matrix.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "python efs/cpv/gen_cnv_matrix.py /home/ec2-user/SageMaker/efs/cpv/data files_meta_cpv.csv cpv.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download Data from GDC repository\n",
    "mkdir /tmp/gdcData/ <br>\n",
    "cd /tmp/gdcData/ <br>\n",
    "/home/ec2-user/SageMaker/gdc-client download -m /home/ec2-user/SageMaker/manifest.txt<br>\n",
    "\n",
    "#### Check data integrity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_as_bytes(file):\n",
    "    with file:\n",
    "        return file.read()\n",
    "\n",
    "def check(dirname,manifest_file,total):\n",
    "    '''\n",
    "    check the md5 for each file downloaded. if md5 does not match, report error.\n",
    "    '''\n",
    "    df = pd.read_csv(manifest_file,sep='\\t')\n",
    "    count = 0\n",
    "    for idname in os.listdir(dirname):\n",
    "        # list all the ids \n",
    "        if idname.find(\"-\") != -1:\n",
    "            idpath = dirname +\"/\" + idname\n",
    "\n",
    "            for filename in os.listdir(idpath):\n",
    "                # check the miRNA file\n",
    "                if filename.find(\"-\") != -1:\n",
    "                    filepath = idpath + \"/\" + filename\n",
    "                    filehash = hashlib.md5(file_as_bytes(open(filepath, 'rb'))).hexdigest()\n",
    "                    if df.loc[df['filename'] == filename].md5.values[0] != filehash:\n",
    "                        print(\"file id {} download fails, please downlaod again\".format(idname))\n",
    "#                         logger.info(\"file id {} download fails, please downlaod again\".format(idname))\n",
    "                    else:\n",
    "                        count +=1\n",
    "    if count == total:\n",
    "        print(\"successful downloads\")\n",
    "#         logger.info(\"successful downloads\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check(\"efs/dna_meth/data\",\"efs/dna_meth/gdc_manifest.2018-11-07-dna_meth.txt\",12359)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate DNA Mehtylation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractMatrix(dirname):\n",
    "    '''\n",
    "    return a dataframe of the miRNA matrix, each row is the miRNA counts for a file_id\n",
    "\n",
    "    '''\n",
    "    count = 0\n",
    "\n",
    "    miRNA_data = []\n",
    "    for idname in os.listdir(dirname):\n",
    "        # list all the ids \n",
    "        if idname.find(\"-\") != -1:\n",
    "            idpath = dirname +\"/\" + idname\n",
    "\n",
    "            # all the files in each id directory\n",
    "            for filename in os.listdir(idpath):\n",
    "                # check the miRNA file\n",
    "                if filename.find(\"-\") != -1:\n",
    "                    filepath = idpath + \"/\" + filename\n",
    "                    df = pd.read_csv(filepath,sep=\"\\t\")\n",
    "                    # columns = [\"miRNA_ID\", \"read_count\"]\n",
    "                    if count ==0:\n",
    "                        # get the miRNA_IDs \n",
    "                        miRNA_IDs = df.Beta_value.values.tolist()\n",
    "\n",
    "                    id_miRNA_read_counts = [idname] + df.read_count.values.tolist()\n",
    "                    miRNA_data.append(id_miRNA_read_counts)\n",
    "\n",
    "\n",
    "                    count +=1\n",
    "                    # print (df)\n",
    "    columns = [\"file_id\"] + miRNA_IDs\n",
    "    df = pd.DataFrame(miRNA_data, columns=columns)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_df = extractMatrix(\"efs/dna_meth/data\")\n",
    "matrix_df.to_csv(\"da_meth.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create S3 Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_region = boto3.session.Session().region_name # set the region of the instance\n",
    "print(my_region)\n",
    "number = str(random.randint(0,1000))\n",
    "bucket_name=\"gdcdataml\"+str(number) # <--- change this variable to a unique name for your bucket\n",
    "s3 = boto3.client('s3')\n",
    "print(\"bucketname:\",bucket_name)\n",
    "try:\n",
    "    if  my_region == 'us-east-1':\n",
    "      s3.create_bucket(Bucket=bucket_name)\n",
    "    else: \n",
    "      s3.create_bucket(Bucket=bucket_name, CreateBucketConfiguration={ 'LocationConstraint': my_region })\n",
    "    print('S3 bucket created successfully')\n",
    "except Exception as e:\n",
    "    print('S3 error: ',e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cwd = os.getcwd()\n",
    "# print(cwd)\n",
    "print(4*\"=\"+\"start checking\"+4*\"=\")\n",
    "manifest_file = cwd+'/gdc_manifest.2018-11-07-all.txt'\n",
    "df = pd.read_csv(manifest_file,sep='\\t')\n",
    "total = df.shape[0]\n",
    "check(manifest_file,total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uploadDirectory(path,bucketname):\n",
    "    for root,dirs,files in os.walk(path):\n",
    "        for file in files:\n",
    "            s3.upload_file(os.path.join(root,file),bucketname,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_dir = '/tmp/gdcData/'\n",
    "uploadDirectory(raw_data_dir,bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix = dict()\n",
    "# inventory = dict()\n",
    "# for each_record in data:\n",
    "#     file_id = each_record['file_id']\n",
    "#     case_id =  each_record['cases'][0]['case_id']\n",
    "#     filename = each_record['file_name']\n",
    "    \n",
    "#     if case_id in case_ids:\n",
    "#         case_ids.add(case_id)\n",
    "\n",
    "#     else:\n",
    "\n",
    "#         matrix.append([file_id,case_id])\n",
    "#         inventory[case_id]=[0]*8\n",
    "    \n",
    "#     if case_id not in matrix:\n",
    "#         matrix[case_id]= []\n",
    "#         inventory[case_id]=[0]*8\n",
    "    \n",
    "#     matrix[case_id].append(file_id)\n",
    "        \n",
    "#     if \".grch38.seg.v2.txt\" in filename: # Copy Num Variation \n",
    "#         inventory[case_id][0]+=1\n",
    "#     elif \"nocnv_grch38.seg.v2.txt\" in filename: # Masked Copy Num Variation\n",
    "#         inventory[case_id][1]+=1\n",
    "#     elif \"hg38.txt\" in filename: # DNA Methylation \n",
    "#         inventory[case_id][2]+=1\n",
    "#     elif \"isoforms.quantification.txt\" in filename: # isoform\n",
    "#         inventory[case_id][3]+=1\n",
    "#     elif \"mirnas.quantification.txt\" in filename: # miRNA\n",
    "#         inventory[case_id][4]+=1\n",
    "#     elif \"FPKM.txt.gz\" in filename: # normalized mRNA Gene Expression\n",
    "#         inventory[case_id][5]+=1\n",
    "#     elif \"FPKM-UQ.txt.gz\" in filename: # 75th percentile mRNA Gene Expression\n",
    "#         inventory[case_id][6]+=1\n",
    "#     elif \"htseq.counts.gz\" in filename: # HT-seq mRNA Gene Expression\n",
    "#         inventory[case_id][7]+=1\n",
    "        \n",
    "\n",
    "# df = pd.DataFrame.from_dict(matrix, orient='index').reset_index()\n",
    "# df2 = pd.DataFrame.from_dict(inventory, orient='index').reset_index()\n",
    "# df2.columns =['case_id','CNV', 'MCNV', 'DNA-MYL', 'ISO', 'miRNA', 'mRNA_norm', 'mRNA_75','mRNA_HT']\n",
    "# print(df.shape)\n",
    "# # filter = (df[\"CNV\"] != \"\")  & (df[\"DNA-MYL\"] != \"\") & (df[\"mRNA_norm\"] != \"\") & (df[\"miRNA\"] != \"\")\n",
    "# # dfNew = df[filter]\n",
    "# # print(dfNew.shape)\n",
    "# df.to_csv(outputfile,index=False)\n",
    "# df2.to_csv(\"inventory.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = '/tmp/test'\n",
    "boto3.Session().resource('s3').Bucket(bucket_name).Object(os.path.join(prefix, '')).upload_file('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd = open(\"files_meta_cpv.csv\",'w')\n",
    "cases_endpt = 'https://api.gdc.cancer.gov/cases'\n",
    "\n",
    "fields = [\n",
    "    \"files.file_id\",\n",
    "    \"files.file_name\",\n",
    "#     \"aliquot_ids\",\n",
    "#     \"analyte_ids\",\n",
    "#     \"case_id\",\n",
    "#     \"project.project_id\",\n",
    "    \"sample_ids\",\n",
    "#     \"files.cases.project.project_id\",\n",
    "#     \"files.cases.project.program.name\",\n",
    "#     \"primary_site\",\n",
    "#     \"disease_type\",\n",
    "#     \"diagnoses.vital_status\",\n",
    "#     \"demographic.gender\",\n",
    "#     \"demographic.race\",\n",
    "#     \"demographic.year_of_birth\"\n",
    "#     \"exposures.bmi\",\n",
    "    # \"exposures.height\",\n",
    "    # \"exposures.weight\",\n",
    "    # \"exposures.cigarettes_per_day\",\n",
    "    # \"exposures.alcohol_history\",\n",
    "    # \"exposures.alcohol_intensity\",\n",
    "    # \"exposures.years_smoked\"\n",
    "    ]\n",
    "\n",
    "\n",
    "filters = {\n",
    "    \"op\":\"in\",\n",
    "    \"content\":{\n",
    "        \"field\":\"file_id\",\n",
    "        \"value\": file_ids[-5:].tolist()\n",
    "    }\n",
    "}\n",
    "\n",
    "# print (filters)\n",
    "fields = ','.join(fields)\n",
    "#expand group is diagnosis and demoragphic\n",
    "params = {\n",
    "    \"filters\" : filters,\n",
    "    \"fields\": fields,\n",
    "    # \"expand\" : \"diagnoses,demographic,exposures\",\n",
    "    \"format\": \"CSV\",\n",
    "    \"pretty\": \"true\",\n",
    "    \"size\": size\n",
    "}\n",
    "# print (params)\n",
    "#print (filters)\n",
    "#print (fields)\n",
    "\n",
    "\n",
    "response = requests.post(cases_endpt, headers = {\"Content-Type\": \"application/json\"},json = params)\n",
    "# print (response.content.decode(\"utf-8\"))\n",
    "fd.write(response.content.decode(\"utf-8\"))\n",
    "fd.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
